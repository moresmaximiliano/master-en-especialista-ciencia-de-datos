{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 3. Multilayer Perceptron</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>3. Evaluar el rendimiento</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Evaluar empíricamente las configuraciones de red](#section1)\n",
    "* [2. División de datos](#section2)\n",
    "    * [2.1. Verificación automática](#section2.1)\n",
    "    * [2.2. Verificación manual](#section2.2)\n",
    "* [3. Validación cruzada](#section3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta lección, descubrirá algunas formas que puede utilizar para evaluar el rendimiento del modelo. Después de completar esta lección, sabrá:\n",
    "* Cómo evaluar un modelo utilizando un conjunto de datos de verificación automática.\n",
    "* Cómo evaluar un modelo utilizando un conjunto de datos de verificación manual.\n",
    "* Cómo evaluar un modelo mediante la validación cruzada de k-fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# La mayoría de decisiones sobre el modelo de DL y ML deben resolverse empíricamente (prueba y error)\n",
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Evaluar empíricamente las configuraciones de red</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A la hora de evaluar modelos tenemos que tener en cuenta decisiones de nivel inferior como la elección de la función de pérdida, funciones de activación, procedimiento de optimización y número de épocas, además de la longitud y profundidad de nuestras capas ocultas (podemos copiar estructuras de redes de otras personas para trabajos similares). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. División de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es común utilizar una simple separación de datos en conjuntos de datos de entrenamiento y validación. Keras proporciona dos formas convenientes de evaluar sus algoritmos de Deep Learning de esta manera:\n",
    "1. Utilice un conjunto de datos de verificación automática.\n",
    "2. Utilice un conjunto de datos de verificación manual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.1. Verificación automática</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras puede separar una parte de sus datos de entrenamiento/validación y evaluar el rendimiento del modelo en cada época. Puede hacer esto configurando el argumento `validation_split` en la función `fit()` en un porcentaje del tamaño de su conjunto de datos de entrenamiento. \n",
    "\n",
    "Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.4709 - loss: 2.6988 - val_accuracy: 0.5354 - val_loss: 1.4609\n",
      "Epoch 2/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5369 - loss: 1.4441 - val_accuracy: 0.5906 - val_loss: 1.0080\n",
      "Epoch 3/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5491 - loss: 1.0606 - val_accuracy: 0.5512 - val_loss: 0.8169\n",
      "Epoch 4/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4793 - loss: 0.9887 - val_accuracy: 0.5551 - val_loss: 0.7829\n",
      "Epoch 5/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5501 - loss: 0.8049 - val_accuracy: 0.4370 - val_loss: 0.8761\n",
      "Epoch 6/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4842 - loss: 0.8435 - val_accuracy: 0.6535 - val_loss: 0.8427\n",
      "Epoch 7/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5460 - loss: 0.8483 - val_accuracy: 0.5630 - val_loss: 0.7771\n",
      "Epoch 8/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5189 - loss: 0.8072 - val_accuracy: 0.6496 - val_loss: 0.8105\n",
      "Epoch 9/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5781 - loss: 0.7852 - val_accuracy: 0.6417 - val_loss: 0.7590\n",
      "Epoch 10/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5641 - loss: 0.7161 - val_accuracy: 0.5906 - val_loss: 0.7498\n",
      "Epoch 11/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5626 - loss: 0.7595 - val_accuracy: 0.6339 - val_loss: 0.7428\n",
      "Epoch 12/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5685 - loss: 0.8602 - val_accuracy: 0.6142 - val_loss: 0.7255\n",
      "Epoch 13/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5805 - loss: 0.7314 - val_accuracy: 0.6063 - val_loss: 0.7221\n",
      "Epoch 14/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5822 - loss: 0.7620 - val_accuracy: 0.5354 - val_loss: 0.7921\n",
      "Epoch 15/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5478 - loss: 0.7393 - val_accuracy: 0.6693 - val_loss: 0.7897\n",
      "Epoch 16/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5504 - loss: 0.7793 - val_accuracy: 0.6614 - val_loss: 0.7574\n",
      "Epoch 17/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5799 - loss: 0.7300 - val_accuracy: 0.5945 - val_loss: 0.7318\n",
      "Epoch 18/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5370 - loss: 0.7213 - val_accuracy: 0.6417 - val_loss: 0.7115\n",
      "Epoch 19/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5766 - loss: 0.6967 - val_accuracy: 0.6575 - val_loss: 0.7342\n",
      "Epoch 20/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6230 - loss: 0.7159 - val_accuracy: 0.5591 - val_loss: 0.7596\n",
      "Epoch 21/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5491 - loss: 0.7118 - val_accuracy: 0.6299 - val_loss: 0.6995\n",
      "Epoch 22/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6018 - loss: 0.6976 - val_accuracy: 0.5866 - val_loss: 0.7386\n",
      "Epoch 23/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6145 - loss: 0.6893 - val_accuracy: 0.6575 - val_loss: 0.7118\n",
      "Epoch 24/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6218 - loss: 0.6815 - val_accuracy: 0.6614 - val_loss: 0.6866\n",
      "Epoch 25/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6943 - loss: 0.6446 - val_accuracy: 0.6063 - val_loss: 0.7205\n",
      "Epoch 26/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6346 - loss: 0.6610 - val_accuracy: 0.6457 - val_loss: 0.6807\n",
      "Epoch 27/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6512 - loss: 0.6329 - val_accuracy: 0.4882 - val_loss: 0.9018\n",
      "Epoch 28/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5639 - loss: 0.7428 - val_accuracy: 0.6260 - val_loss: 0.7133\n",
      "Epoch 29/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6401 - loss: 0.6483 - val_accuracy: 0.6693 - val_loss: 0.7002\n",
      "Epoch 30/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6682 - loss: 0.6145 - val_accuracy: 0.6732 - val_loss: 0.6772\n",
      "Epoch 31/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6430 - loss: 0.7003 - val_accuracy: 0.5906 - val_loss: 0.7641\n",
      "Epoch 32/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6158 - loss: 0.6624 - val_accuracy: 0.4961 - val_loss: 0.9049\n",
      "Epoch 33/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6082 - loss: 0.6699 - val_accuracy: 0.6772 - val_loss: 0.6768\n",
      "Epoch 34/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6086 - loss: 0.7917 - val_accuracy: 0.5669 - val_loss: 0.7997\n",
      "Epoch 35/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6049 - loss: 0.7025 - val_accuracy: 0.6693 - val_loss: 0.7105\n",
      "Epoch 36/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5884 - loss: 0.6815 - val_accuracy: 0.6299 - val_loss: 0.6832\n",
      "Epoch 37/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6598 - loss: 0.6126 - val_accuracy: 0.6614 - val_loss: 0.6722\n",
      "Epoch 38/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6646 - loss: 0.6166 - val_accuracy: 0.6732 - val_loss: 0.6760\n",
      "Epoch 39/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6275 - loss: 0.6793 - val_accuracy: 0.6693 - val_loss: 0.7387\n",
      "Epoch 40/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6239 - loss: 0.7153 - val_accuracy: 0.6535 - val_loss: 0.7204\n",
      "Epoch 41/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5857 - loss: 0.7013 - val_accuracy: 0.6457 - val_loss: 0.6951\n",
      "Epoch 42/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6490 - loss: 0.6466 - val_accuracy: 0.6732 - val_loss: 0.6764\n",
      "Epoch 43/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6212 - loss: 0.7336 - val_accuracy: 0.6417 - val_loss: 0.6794\n",
      "Epoch 44/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6620 - loss: 0.6116 - val_accuracy: 0.6614 - val_loss: 0.6929\n",
      "Epoch 45/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6675 - loss: 0.6030 - val_accuracy: 0.6654 - val_loss: 0.7849\n",
      "Epoch 46/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6577 - loss: 0.6790 - val_accuracy: 0.6496 - val_loss: 0.6610\n",
      "Epoch 47/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6874 - loss: 0.6098 - val_accuracy: 0.6811 - val_loss: 0.6527\n",
      "Epoch 48/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6713 - loss: 0.6365 - val_accuracy: 0.6378 - val_loss: 0.6853\n",
      "Epoch 49/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6855 - loss: 0.5934 - val_accuracy: 0.6772 - val_loss: 0.6799\n",
      "Epoch 50/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6843 - loss: 0.5925 - val_accuracy: 0.6811 - val_loss: 0.6624\n",
      "Epoch 51/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6389 - loss: 0.6503 - val_accuracy: 0.6850 - val_loss: 0.6619\n",
      "Epoch 52/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6581 - loss: 0.6507 - val_accuracy: 0.6811 - val_loss: 0.6628\n",
      "Epoch 53/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6733 - loss: 0.6114 - val_accuracy: 0.6693 - val_loss: 0.6699\n",
      "Epoch 54/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6346 - loss: 0.6769 - val_accuracy: 0.6457 - val_loss: 0.6554\n",
      "Epoch 55/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6880 - loss: 0.6055 - val_accuracy: 0.6693 - val_loss: 0.6484\n",
      "Epoch 56/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6803 - loss: 0.5903 - val_accuracy: 0.6732 - val_loss: 0.6537\n",
      "Epoch 57/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6452 - loss: 0.6089 - val_accuracy: 0.6772 - val_loss: 0.6613\n",
      "Epoch 58/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6766 - loss: 0.6381 - val_accuracy: 0.6732 - val_loss: 0.6798\n",
      "Epoch 59/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7023 - loss: 0.5849 - val_accuracy: 0.6890 - val_loss: 0.6572\n",
      "Epoch 60/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6423 - loss: 0.6253 - val_accuracy: 0.6732 - val_loss: 0.6692\n",
      "Epoch 61/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7033 - loss: 0.5789 - val_accuracy: 0.6811 - val_loss: 0.6605\n",
      "Epoch 62/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6393 - loss: 0.7590 - val_accuracy: 0.6339 - val_loss: 0.7264\n",
      "Epoch 63/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6117 - loss: 0.6966 - val_accuracy: 0.6457 - val_loss: 0.7188\n",
      "Epoch 64/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6688 - loss: 0.5964 - val_accuracy: 0.7008 - val_loss: 0.6416\n",
      "Epoch 65/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6518 - loss: 0.6093 - val_accuracy: 0.6693 - val_loss: 0.6852\n",
      "Epoch 66/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6684 - loss: 0.6568 - val_accuracy: 0.6850 - val_loss: 0.6574\n",
      "Epoch 67/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6710 - loss: 0.5929 - val_accuracy: 0.6378 - val_loss: 0.6949\n",
      "Epoch 68/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6481 - loss: 0.6027 - val_accuracy: 0.6772 - val_loss: 0.6730\n",
      "Epoch 69/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6890 - loss: 0.5789 - val_accuracy: 0.6693 - val_loss: 0.6663\n",
      "Epoch 70/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7013 - loss: 0.5739 - val_accuracy: 0.6732 - val_loss: 0.6491\n",
      "Epoch 71/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7160 - loss: 0.5485 - val_accuracy: 0.6732 - val_loss: 0.6560\n",
      "Epoch 72/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6543 - loss: 0.6175 - val_accuracy: 0.6496 - val_loss: 0.7066\n",
      "Epoch 73/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6737 - loss: 0.6286 - val_accuracy: 0.6811 - val_loss: 0.6824\n",
      "Epoch 74/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7103 - loss: 0.5852 - val_accuracy: 0.6654 - val_loss: 0.7279\n",
      "Epoch 75/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6774 - loss: 0.6367 - val_accuracy: 0.6732 - val_loss: 0.6796\n",
      "Epoch 76/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6236 - loss: 0.6701 - val_accuracy: 0.6654 - val_loss: 0.7184\n",
      "Epoch 77/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6515 - loss: 0.6468 - val_accuracy: 0.6890 - val_loss: 0.6769\n",
      "Epoch 78/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6994 - loss: 0.5768 - val_accuracy: 0.6772 - val_loss: 0.6652\n",
      "Epoch 79/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7346 - loss: 0.5390 - val_accuracy: 0.6417 - val_loss: 0.7559\n",
      "Epoch 80/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6431 - loss: 0.6373 - val_accuracy: 0.6929 - val_loss: 0.6576\n",
      "Epoch 81/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7032 - loss: 0.5603 - val_accuracy: 0.6929 - val_loss: 0.6615\n",
      "Epoch 82/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7241 - loss: 0.5662 - val_accuracy: 0.6929 - val_loss: 0.6575\n",
      "Epoch 83/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6603 - loss: 0.5948 - val_accuracy: 0.6850 - val_loss: 0.6690\n",
      "Epoch 84/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7098 - loss: 0.5509 - val_accuracy: 0.5354 - val_loss: 0.9590\n",
      "Epoch 85/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6374 - loss: 0.6636 - val_accuracy: 0.5787 - val_loss: 0.8430\n",
      "Epoch 86/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6637 - loss: 0.6163 - val_accuracy: 0.6575 - val_loss: 0.7151\n",
      "Epoch 87/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.6179 - val_accuracy: 0.6732 - val_loss: 0.6689\n",
      "Epoch 88/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6953 - loss: 0.5776 - val_accuracy: 0.6654 - val_loss: 0.6743\n",
      "Epoch 89/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6861 - loss: 0.5689 - val_accuracy: 0.6811 - val_loss: 0.6823\n",
      "Epoch 90/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6841 - loss: 0.5920 - val_accuracy: 0.6732 - val_loss: 0.6872\n",
      "Epoch 91/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7018 - loss: 0.5807 - val_accuracy: 0.6614 - val_loss: 0.7645\n",
      "Epoch 92/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7209 - loss: 0.5979 - val_accuracy: 0.6772 - val_loss: 0.6531\n",
      "Epoch 93/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6974 - loss: 0.5640 - val_accuracy: 0.6654 - val_loss: 0.8134\n",
      "Epoch 94/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6587 - loss: 0.6739 - val_accuracy: 0.6732 - val_loss: 0.6800\n",
      "Epoch 95/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7283 - loss: 0.5575 - val_accuracy: 0.6732 - val_loss: 0.6755\n",
      "Epoch 96/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6850 - loss: 0.5949 - val_accuracy: 0.6496 - val_loss: 0.7180\n",
      "Epoch 97/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6695 - loss: 0.6591 - val_accuracy: 0.6890 - val_loss: 0.6504\n",
      "Epoch 98/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6844 - loss: 0.5964 - val_accuracy: 0.6220 - val_loss: 0.7891\n",
      "Epoch 99/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7227 - loss: 0.5464 - val_accuracy: 0.6614 - val_loss: 0.6650\n",
      "Epoch 100/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7222 - loss: 0.5468 - val_accuracy: 0.6772 - val_loss: 0.6744\n",
      "Epoch 101/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7199 - loss: 0.5737 - val_accuracy: 0.7008 - val_loss: 0.6605\n",
      "Epoch 102/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6606 - loss: 0.5762 - val_accuracy: 0.6969 - val_loss: 0.6407\n",
      "Epoch 103/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7161 - loss: 0.5602 - val_accuracy: 0.6772 - val_loss: 0.6730\n",
      "Epoch 104/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7356 - loss: 0.5305 - val_accuracy: 0.6378 - val_loss: 0.7579\n",
      "Epoch 105/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6556 - loss: 0.6340 - val_accuracy: 0.6890 - val_loss: 0.6524\n",
      "Epoch 106/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7333 - loss: 0.5333 - val_accuracy: 0.6772 - val_loss: 0.6834\n",
      "Epoch 107/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6749 - loss: 0.6101 - val_accuracy: 0.6772 - val_loss: 0.6596\n",
      "Epoch 108/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6475 - loss: 0.5784 - val_accuracy: 0.6929 - val_loss: 0.6480\n",
      "Epoch 109/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7021 - loss: 0.5708 - val_accuracy: 0.6929 - val_loss: 0.6621\n",
      "Epoch 110/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7020 - loss: 0.5908 - val_accuracy: 0.6142 - val_loss: 0.8097\n",
      "Epoch 111/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6414 - loss: 0.6500 - val_accuracy: 0.6811 - val_loss: 0.6522\n",
      "Epoch 112/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7312 - loss: 0.5368 - val_accuracy: 0.6811 - val_loss: 0.6648\n",
      "Epoch 113/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7260 - loss: 0.5790 - val_accuracy: 0.6811 - val_loss: 0.6624\n",
      "Epoch 114/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6958 - loss: 0.5563 - val_accuracy: 0.6732 - val_loss: 0.6708\n",
      "Epoch 115/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6911 - loss: 0.5655 - val_accuracy: 0.6929 - val_loss: 0.6547\n",
      "Epoch 116/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7114 - loss: 0.5498 - val_accuracy: 0.6772 - val_loss: 0.6768\n",
      "Epoch 117/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6302 - loss: 0.6246 - val_accuracy: 0.6575 - val_loss: 0.7262\n",
      "Epoch 118/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6802 - loss: 0.6061 - val_accuracy: 0.6890 - val_loss: 0.6528\n",
      "Epoch 119/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7030 - loss: 0.5875 - val_accuracy: 0.6732 - val_loss: 0.6681\n",
      "Epoch 120/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6997 - loss: 0.5797 - val_accuracy: 0.6929 - val_loss: 0.6487\n",
      "Epoch 121/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7201 - loss: 0.5330 - val_accuracy: 0.7008 - val_loss: 0.6696\n",
      "Epoch 122/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6904 - loss: 0.5750 - val_accuracy: 0.6811 - val_loss: 0.6504\n",
      "Epoch 123/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7122 - loss: 0.5420 - val_accuracy: 0.6732 - val_loss: 0.6494\n",
      "Epoch 124/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6890 - loss: 0.5636 - val_accuracy: 0.6693 - val_loss: 0.7120\n",
      "Epoch 125/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7033 - loss: 0.5479 - val_accuracy: 0.6929 - val_loss: 0.6627\n",
      "Epoch 126/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7095 - loss: 0.5524 - val_accuracy: 0.7008 - val_loss: 0.6521\n",
      "Epoch 127/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7184 - loss: 0.5362 - val_accuracy: 0.6535 - val_loss: 0.6874\n",
      "Epoch 128/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7055 - loss: 0.5668 - val_accuracy: 0.6732 - val_loss: 0.6718\n",
      "Epoch 129/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7061 - loss: 0.5438 - val_accuracy: 0.6654 - val_loss: 0.6739\n",
      "Epoch 130/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7313 - loss: 0.5501 - val_accuracy: 0.6457 - val_loss: 0.7256\n",
      "Epoch 131/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7013 - loss: 0.6357 - val_accuracy: 0.6969 - val_loss: 0.6289\n",
      "Epoch 132/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 0.5809 - val_accuracy: 0.6772 - val_loss: 0.6452\n",
      "Epoch 133/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7342 - loss: 0.5364 - val_accuracy: 0.7008 - val_loss: 0.6498\n",
      "Epoch 134/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7094 - loss: 0.5545 - val_accuracy: 0.6732 - val_loss: 0.6542\n",
      "Epoch 135/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7302 - loss: 0.5355 - val_accuracy: 0.6850 - val_loss: 0.6505\n",
      "Epoch 136/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6786 - loss: 0.5725 - val_accuracy: 0.7008 - val_loss: 0.6412\n",
      "Epoch 137/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7222 - loss: 0.5308 - val_accuracy: 0.7165 - val_loss: 0.6495\n",
      "Epoch 138/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7544 - loss: 0.5312 - val_accuracy: 0.6496 - val_loss: 0.6891\n",
      "Epoch 139/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7487 - loss: 0.5093 - val_accuracy: 0.6693 - val_loss: 0.6715\n",
      "Epoch 140/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6493 - loss: 0.6533 - val_accuracy: 0.6575 - val_loss: 0.7223\n",
      "Epoch 141/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7152 - loss: 0.5591 - val_accuracy: 0.6811 - val_loss: 0.6715\n",
      "Epoch 142/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6947 - loss: 0.5562 - val_accuracy: 0.6693 - val_loss: 0.7330\n",
      "Epoch 143/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7159 - loss: 0.5711 - val_accuracy: 0.6850 - val_loss: 0.6783\n",
      "Epoch 144/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7038 - loss: 0.5562 - val_accuracy: 0.7008 - val_loss: 0.6527\n",
      "Epoch 145/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6877 - loss: 0.5795 - val_accuracy: 0.6732 - val_loss: 0.6613\n",
      "Epoch 146/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7230 - loss: 0.5212 - val_accuracy: 0.6654 - val_loss: 0.6651\n",
      "Epoch 147/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6526 - loss: 0.6166 - val_accuracy: 0.6850 - val_loss: 0.6485\n",
      "Epoch 148/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7404 - loss: 0.5187 - val_accuracy: 0.6772 - val_loss: 0.6528\n",
      "Epoch 149/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7283 - loss: 0.5238 - val_accuracy: 0.6890 - val_loss: 0.6911\n",
      "Epoch 150/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6634 - loss: 0.6473 - val_accuracy: 0.6732 - val_loss: 0.6613\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19f5ed82a50>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Por lo tedioso del proceso de entrenamiento de modelos, es normal utilizar una separación en porcentaje de los datos para el entrenamiento y evaluación del mismo\n",
    "# Podemos ver también el Accuracy y pérdida en cada época con este modelo\n",
    "\n",
    "# MLP con set de validación automática (con División por Porcentajes)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el set de pima-indians\n",
    "dataset = np.loadtxt('Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "# Dividimos los datos en imputs (características) y outputs (clase)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# Creamos el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu')) # primera capa oculta (si encontramos una mejor configuración cambiar)\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# Compilamos el modelo\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# Entrenamos el modelo\n",
    "model.fit(X,Y,validation_split=0.33,epochs=150, batch_size=10) # le decimos que el 33% de los datos se van a usar para validación y el 67% para entrenamiento\n",
    "# El accuracy final del modelo es el mostrado por el accuracy de la última época"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.2. Verificación manual</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras también le permite especificar manualmente el conjunto de datos que se utilizará para la validación durante el entrenamiento. En este ejemplo usamos la práctica función `train_test_split()` de Scikit-learn usando un 67%/33%. \n",
    "\n",
    "El conjunto de datos de validación se puede especificar a la función `fit()` mediante el argumento `validation_data`. Toma una tupla de los conjuntos de datos de entrada (X) y salida (y).\n",
    "\n",
    "Veamos un ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3693 - loss: 102.0112 - val_accuracy: 0.6220 - val_loss: 17.7370\n",
      "Epoch 2/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6650 - loss: 14.0401 - val_accuracy: 0.6063 - val_loss: 13.1766\n",
      "Epoch 3/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6096 - loss: 11.8378 - val_accuracy: 0.5866 - val_loss: 11.0887\n",
      "Epoch 4/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6276 - loss: 9.8255 - val_accuracy: 0.6024 - val_loss: 7.6637\n",
      "Epoch 5/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6035 - loss: 7.8792 - val_accuracy: 0.5787 - val_loss: 4.0873\n",
      "Epoch 6/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5664 - loss: 3.8579 - val_accuracy: 0.5709 - val_loss: 2.8102\n",
      "Epoch 7/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5683 - loss: 3.2575 - val_accuracy: 0.5906 - val_loss: 2.7527\n",
      "Epoch 8/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5695 - loss: 2.2661 - val_accuracy: 0.5787 - val_loss: 3.1203\n",
      "Epoch 9/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5984 - loss: 2.4389 - val_accuracy: 0.5827 - val_loss: 4.4534\n",
      "Epoch 10/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6271 - loss: 2.3569 - val_accuracy: 0.5945 - val_loss: 1.8972\n",
      "Epoch 11/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5990 - loss: 2.0850 - val_accuracy: 0.5945 - val_loss: 2.1328\n",
      "Epoch 12/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5617 - loss: 2.7855 - val_accuracy: 0.4685 - val_loss: 2.7303\n",
      "Epoch 13/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6171 - loss: 2.3140 - val_accuracy: 0.5709 - val_loss: 1.7975\n",
      "Epoch 14/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5723 - loss: 1.8155 - val_accuracy: 0.5945 - val_loss: 1.5477\n",
      "Epoch 15/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6258 - loss: 1.6621 - val_accuracy: 0.5394 - val_loss: 1.7764\n",
      "Epoch 16/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5336 - loss: 2.1990 - val_accuracy: 0.6299 - val_loss: 1.5927\n",
      "Epoch 17/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5832 - loss: 1.7196 - val_accuracy: 0.6220 - val_loss: 1.4565\n",
      "Epoch 18/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6243 - loss: 1.5812 - val_accuracy: 0.6575 - val_loss: 1.3476\n",
      "Epoch 19/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5764 - loss: 1.8341 - val_accuracy: 0.6063 - val_loss: 1.2057\n",
      "Epoch 20/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6118 - loss: 1.6143 - val_accuracy: 0.4724 - val_loss: 2.5875\n",
      "Epoch 21/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6417 - loss: 1.5563 - val_accuracy: 0.6142 - val_loss: 1.1810\n",
      "Epoch 22/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6253 - loss: 1.4944 - val_accuracy: 0.6063 - val_loss: 2.0238\n",
      "Epoch 23/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5872 - loss: 1.6502 - val_accuracy: 0.6063 - val_loss: 1.9483\n",
      "Epoch 24/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5945 - loss: 2.1633 - val_accuracy: 0.6378 - val_loss: 1.0710\n",
      "Epoch 25/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6430 - loss: 1.0854 - val_accuracy: 0.6339 - val_loss: 1.0486\n",
      "Epoch 26/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.9115 - val_accuracy: 0.5118 - val_loss: 1.4445\n",
      "Epoch 27/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6019 - loss: 1.2320 - val_accuracy: 0.6339 - val_loss: 1.0413\n",
      "Epoch 28/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6439 - loss: 0.9752 - val_accuracy: 0.6181 - val_loss: 0.9644\n",
      "Epoch 29/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6603 - loss: 0.9251 - val_accuracy: 0.6220 - val_loss: 0.9151\n",
      "Epoch 30/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6727 - loss: 1.0498 - val_accuracy: 0.4843 - val_loss: 1.5276\n",
      "Epoch 31/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5907 - loss: 1.3248 - val_accuracy: 0.6142 - val_loss: 1.5521\n",
      "Epoch 32/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6588 - loss: 1.0304 - val_accuracy: 0.6220 - val_loss: 0.9390\n",
      "Epoch 33/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5873 - loss: 1.1484 - val_accuracy: 0.6575 - val_loss: 0.9166\n",
      "Epoch 34/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6072 - loss: 1.0784 - val_accuracy: 0.6063 - val_loss: 1.4841\n",
      "Epoch 35/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6892 - loss: 1.0371 - val_accuracy: 0.6496 - val_loss: 0.8714\n",
      "Epoch 36/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6726 - loss: 0.7636 - val_accuracy: 0.5984 - val_loss: 1.3006\n",
      "Epoch 37/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6422 - loss: 1.0058 - val_accuracy: 0.6181 - val_loss: 1.1190\n",
      "Epoch 38/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6488 - loss: 1.0197 - val_accuracy: 0.6220 - val_loss: 0.9191\n",
      "Epoch 39/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6077 - loss: 0.9328 - val_accuracy: 0.6535 - val_loss: 0.8042\n",
      "Epoch 40/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6967 - loss: 0.8096 - val_accuracy: 0.6063 - val_loss: 1.0371\n",
      "Epoch 41/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6740 - loss: 0.8339 - val_accuracy: 0.6220 - val_loss: 0.8449\n",
      "Epoch 42/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6058 - loss: 0.9117 - val_accuracy: 0.6339 - val_loss: 0.8109\n",
      "Epoch 43/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6519 - loss: 0.9020 - val_accuracy: 0.6614 - val_loss: 0.8030\n",
      "Epoch 44/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6699 - loss: 0.9289 - val_accuracy: 0.5906 - val_loss: 2.8571\n",
      "Epoch 45/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6470 - loss: 1.5025 - val_accuracy: 0.6220 - val_loss: 0.8231\n",
      "Epoch 46/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6166 - loss: 0.8891 - val_accuracy: 0.6181 - val_loss: 0.9193\n",
      "Epoch 47/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6504 - loss: 0.8767 - val_accuracy: 0.6693 - val_loss: 0.7223\n",
      "Epoch 48/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6545 - loss: 0.9641 - val_accuracy: 0.5591 - val_loss: 0.9947\n",
      "Epoch 49/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6679 - loss: 0.8130 - val_accuracy: 0.6142 - val_loss: 0.9132\n",
      "Epoch 50/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6555 - loss: 0.7534 - val_accuracy: 0.6220 - val_loss: 0.8055\n",
      "Epoch 51/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6621 - loss: 0.8454 - val_accuracy: 0.6063 - val_loss: 1.0753\n",
      "Epoch 52/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6332 - loss: 0.8489 - val_accuracy: 0.6339 - val_loss: 0.7632\n",
      "Epoch 53/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6962 - loss: 0.9823 - val_accuracy: 0.6063 - val_loss: 1.3120\n",
      "Epoch 54/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6553 - loss: 1.1480 - val_accuracy: 0.5984 - val_loss: 2.2282\n",
      "Epoch 55/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6983 - loss: 0.8291 - val_accuracy: 0.6496 - val_loss: 0.7293\n",
      "Epoch 56/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6244 - loss: 0.8017 - val_accuracy: 0.6339 - val_loss: 0.8100\n",
      "Epoch 57/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6514 - loss: 0.8257 - val_accuracy: 0.6732 - val_loss: 0.7441\n",
      "Epoch 58/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6611 - loss: 0.6876 - val_accuracy: 0.5079 - val_loss: 1.1934\n",
      "Epoch 59/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5852 - loss: 0.9574 - val_accuracy: 0.6693 - val_loss: 0.7611\n",
      "Epoch 60/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6814 - loss: 0.7207 - val_accuracy: 0.6102 - val_loss: 2.0400\n",
      "Epoch 61/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6152 - loss: 1.6624 - val_accuracy: 0.6260 - val_loss: 1.0920\n",
      "Epoch 62/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6841 - loss: 0.7742 - val_accuracy: 0.6220 - val_loss: 1.0460\n",
      "Epoch 63/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6552 - loss: 0.8856 - val_accuracy: 0.5787 - val_loss: 0.8994\n",
      "Epoch 64/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6825 - loss: 0.8512 - val_accuracy: 0.5354 - val_loss: 1.1991\n",
      "Epoch 65/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6249 - loss: 0.9312 - val_accuracy: 0.4843 - val_loss: 1.5880\n",
      "Epoch 66/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6665 - loss: 0.8164 - val_accuracy: 0.6299 - val_loss: 0.9963\n",
      "Epoch 67/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6398 - loss: 0.9372 - val_accuracy: 0.7008 - val_loss: 0.7143\n",
      "Epoch 68/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6392 - loss: 0.9245 - val_accuracy: 0.5551 - val_loss: 0.9969\n",
      "Epoch 69/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6381 - loss: 1.1565 - val_accuracy: 0.6732 - val_loss: 0.6709\n",
      "Epoch 70/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6556 - loss: 0.7375 - val_accuracy: 0.6417 - val_loss: 0.7930\n",
      "Epoch 71/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6431 - loss: 0.9810 - val_accuracy: 0.6181 - val_loss: 1.1566\n",
      "Epoch 72/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7055 - loss: 0.6882 - val_accuracy: 0.6693 - val_loss: 0.7506\n",
      "Epoch 73/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6977 - loss: 0.6897 - val_accuracy: 0.6063 - val_loss: 1.6679\n",
      "Epoch 74/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6351 - loss: 1.1040 - val_accuracy: 0.6260 - val_loss: 0.9109\n",
      "Epoch 75/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6528 - loss: 0.8265 - val_accuracy: 0.6417 - val_loss: 0.7536\n",
      "Epoch 76/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6904 - loss: 0.6795 - val_accuracy: 0.6535 - val_loss: 0.7226\n",
      "Epoch 77/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7185 - loss: 0.6662 - val_accuracy: 0.6142 - val_loss: 0.9255\n",
      "Epoch 78/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7086 - loss: 0.7089 - val_accuracy: 0.6693 - val_loss: 0.7769\n",
      "Epoch 79/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6340 - loss: 0.8487 - val_accuracy: 0.6063 - val_loss: 1.5434\n",
      "Epoch 80/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6726 - loss: 0.7594 - val_accuracy: 0.6142 - val_loss: 0.9630\n",
      "Epoch 81/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6351 - loss: 0.9520 - val_accuracy: 0.6339 - val_loss: 0.8155\n",
      "Epoch 82/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6686 - loss: 0.7952 - val_accuracy: 0.6732 - val_loss: 0.8029\n",
      "Epoch 83/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7231 - loss: 0.6476 - val_accuracy: 0.6024 - val_loss: 1.4372\n",
      "Epoch 84/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6816 - loss: 0.9887 - val_accuracy: 0.6850 - val_loss: 0.6700\n",
      "Epoch 85/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6736 - loss: 0.7118 - val_accuracy: 0.6772 - val_loss: 0.6734\n",
      "Epoch 86/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.6758 - val_accuracy: 0.6535 - val_loss: 0.8012\n",
      "Epoch 87/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7312 - loss: 0.6580 - val_accuracy: 0.6181 - val_loss: 0.9350\n",
      "Epoch 88/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6435 - loss: 0.9558 - val_accuracy: 0.6220 - val_loss: 1.2702\n",
      "Epoch 89/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7145 - loss: 0.7473 - val_accuracy: 0.6299 - val_loss: 0.8403\n",
      "Epoch 90/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6914 - loss: 0.7403 - val_accuracy: 0.6496 - val_loss: 0.7220\n",
      "Epoch 91/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6506 - loss: 0.8652 - val_accuracy: 0.4724 - val_loss: 1.6487\n",
      "Epoch 92/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5611 - loss: 1.2212 - val_accuracy: 0.6890 - val_loss: 0.7868\n",
      "Epoch 93/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7238 - loss: 0.6482 - val_accuracy: 0.6142 - val_loss: 1.0509\n",
      "Epoch 94/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6610 - loss: 0.7944 - val_accuracy: 0.6260 - val_loss: 1.0790\n",
      "Epoch 95/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7331 - loss: 0.7170 - val_accuracy: 0.6732 - val_loss: 0.8850\n",
      "Epoch 96/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6508 - loss: 0.9706 - val_accuracy: 0.6260 - val_loss: 0.9995\n",
      "Epoch 97/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6198 - loss: 0.8680 - val_accuracy: 0.6417 - val_loss: 0.8450\n",
      "Epoch 98/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6981 - loss: 0.7378 - val_accuracy: 0.6614 - val_loss: 0.7572\n",
      "Epoch 99/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6836 - loss: 0.7148 - val_accuracy: 0.6417 - val_loss: 0.7986\n",
      "Epoch 100/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 0.6504 - val_accuracy: 0.6732 - val_loss: 0.7053\n",
      "Epoch 101/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7259 - loss: 0.5813 - val_accuracy: 0.7087 - val_loss: 0.6854\n",
      "Epoch 102/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7051 - loss: 0.6874 - val_accuracy: 0.6417 - val_loss: 0.8744\n",
      "Epoch 103/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6770 - loss: 0.7543 - val_accuracy: 0.6693 - val_loss: 0.6922\n",
      "Epoch 104/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7085 - loss: 0.6491 - val_accuracy: 0.7087 - val_loss: 0.6977\n",
      "Epoch 105/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7288 - loss: 0.6464 - val_accuracy: 0.6142 - val_loss: 2.1365\n",
      "Epoch 106/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7005 - loss: 1.0175 - val_accuracy: 0.5866 - val_loss: 0.9919\n",
      "Epoch 107/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6624 - loss: 1.1050 - val_accuracy: 0.6732 - val_loss: 0.6801\n",
      "Epoch 108/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6985 - loss: 0.7175 - val_accuracy: 0.6339 - val_loss: 1.0378\n",
      "Epoch 109/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6735 - loss: 0.7081 - val_accuracy: 0.6969 - val_loss: 0.6657\n",
      "Epoch 110/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6761 - loss: 0.8603 - val_accuracy: 0.6811 - val_loss: 0.6769\n",
      "Epoch 111/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7244 - loss: 0.6336 - val_accuracy: 0.5512 - val_loss: 1.1851\n",
      "Epoch 112/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6752 - loss: 0.9157 - val_accuracy: 0.6654 - val_loss: 0.7617\n",
      "Epoch 113/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7221 - loss: 0.6692 - val_accuracy: 0.5984 - val_loss: 0.8590\n",
      "Epoch 114/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6953 - loss: 0.6877 - val_accuracy: 0.6260 - val_loss: 0.9412\n",
      "Epoch 115/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7142 - loss: 0.9799 - val_accuracy: 0.5984 - val_loss: 1.4340\n",
      "Epoch 116/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6757 - loss: 1.0415 - val_accuracy: 0.6339 - val_loss: 0.9237\n",
      "Epoch 117/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7014 - loss: 0.8386 - val_accuracy: 0.6260 - val_loss: 0.7562\n",
      "Epoch 118/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7060 - loss: 0.7212 - val_accuracy: 0.6220 - val_loss: 0.8307\n",
      "Epoch 119/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6760 - loss: 0.8611 - val_accuracy: 0.6457 - val_loss: 0.8085\n",
      "Epoch 120/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6996 - loss: 0.6905 - val_accuracy: 0.5945 - val_loss: 1.0601\n",
      "Epoch 121/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6637 - loss: 1.0401 - val_accuracy: 0.6654 - val_loss: 0.7804\n",
      "Epoch 122/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7197 - loss: 0.6396 - val_accuracy: 0.6969 - val_loss: 0.6730\n",
      "Epoch 123/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7370 - loss: 0.5244 - val_accuracy: 0.6732 - val_loss: 0.6913\n",
      "Epoch 124/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7283 - loss: 0.6667 - val_accuracy: 0.6063 - val_loss: 1.4061\n",
      "Epoch 125/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7292 - loss: 0.7298 - val_accuracy: 0.6654 - val_loss: 0.7877\n",
      "Epoch 126/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7125 - loss: 0.6260 - val_accuracy: 0.6969 - val_loss: 0.7154\n",
      "Epoch 127/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7423 - loss: 0.5860 - val_accuracy: 0.7047 - val_loss: 0.7518\n",
      "Epoch 128/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7725 - loss: 0.7216 - val_accuracy: 0.6299 - val_loss: 0.8634\n",
      "Epoch 129/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7007 - loss: 0.6368 - val_accuracy: 0.6693 - val_loss: 0.6912\n",
      "Epoch 130/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6784 - loss: 0.7172 - val_accuracy: 0.6299 - val_loss: 1.0317\n",
      "Epoch 131/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6853 - loss: 1.0016 - val_accuracy: 0.6220 - val_loss: 0.9257\n",
      "Epoch 132/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7116 - loss: 0.6592 - val_accuracy: 0.6339 - val_loss: 0.8782\n",
      "Epoch 133/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6490 - loss: 0.8496 - val_accuracy: 0.6929 - val_loss: 0.6919\n",
      "Epoch 134/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7008 - loss: 0.7742 - val_accuracy: 0.6417 - val_loss: 0.9169\n",
      "Epoch 135/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6934 - loss: 0.7373 - val_accuracy: 0.6496 - val_loss: 0.7268\n",
      "Epoch 136/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6676 - loss: 0.7124 - val_accuracy: 0.6063 - val_loss: 1.2580\n",
      "Epoch 137/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7154 - loss: 0.9159 - val_accuracy: 0.6496 - val_loss: 0.9053\n",
      "Epoch 138/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7420 - loss: 0.5593 - val_accuracy: 0.6535 - val_loss: 0.7293\n",
      "Epoch 139/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7360 - loss: 0.6135 - val_accuracy: 0.6772 - val_loss: 0.6811\n",
      "Epoch 140/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6715 - loss: 0.7700 - val_accuracy: 0.6929 - val_loss: 0.7394\n",
      "Epoch 141/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7401 - loss: 0.5998 - val_accuracy: 0.7087 - val_loss: 0.6623\n",
      "Epoch 142/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7393 - loss: 0.6052 - val_accuracy: 0.6890 - val_loss: 0.7882\n",
      "Epoch 143/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6223 - loss: 0.9511 - val_accuracy: 0.6181 - val_loss: 1.1354\n",
      "Epoch 144/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6470 - loss: 0.9971 - val_accuracy: 0.6890 - val_loss: 0.6733\n",
      "Epoch 145/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6668 - loss: 0.7224 - val_accuracy: 0.6378 - val_loss: 0.8281\n",
      "Epoch 146/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7060 - loss: 0.7113 - val_accuracy: 0.6220 - val_loss: 0.7909\n",
      "Epoch 147/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6990 - loss: 0.6744 - val_accuracy: 0.6024 - val_loss: 0.8484\n",
      "Epoch 148/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7214 - loss: 0.6313 - val_accuracy: 0.6772 - val_loss: 0.7158\n",
      "Epoch 149/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7161 - loss: 0.6554 - val_accuracy: 0.6260 - val_loss: 0.8202\n",
      "Epoch 150/150\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7312 - loss: 0.5674 - val_accuracy: 0.6457 - val_loss: 0.9022\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6288 - loss: 0.9316 \n",
      "Accuracy final del modelo: 64.57%\n"
     ]
    }
   ],
   "source": [
    "# Le especificamos manualmente el conjunto de datos usado para entrenamiento (le indicamos con una tupla cuales datos usar de entrada y cuales de salida)\n",
    "\n",
    "# MLP con set de validación manual (con División por Porcentajes)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Cargamos el set de pima-indians\n",
    "dataset = np.loadtxt('Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "# Dividimos los datos en imputs (características) y outputs (clase)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# Dividimos en un 67% para entrenamiento y un 33% para testeo\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33)\n",
    "# Creamos el modelo\n",
    "model = Sequential()\n",
    "model.add(Dense(12,input_dim=8,activation='relu')) # primera capa oculta (si encontramos una mejor configuración cambiar)\n",
    "model.add(Dense(8,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "# Compilamos el modelo\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "# Entrenamos el modelo (ahora con los datos de entrenamiento y pasándole una tupla para que me los identifique, en vez de que me los separe automáticamente)\n",
    "model.fit(X_train,Y_train,validation_data=(X_test,Y_test),epochs=150, batch_size=10) # le decimos que el 33% de los datos se van a usar para validación y el 67% para entrenamiento\n",
    "# Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f'Accuracy final del modelo: {accuracy * 100:.2f}%') # es el de la última época, si queremos que sea constante agregar random_state en la separación de train/test\n",
    "# También podemos analizar en que época verdaderamente se produjo el mayor accuracy (ahora solo vemos como evaluarlo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Validación cruzada</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La validación cruzada a menudo no se usa para evaluar modelos de aprendizaje profundo debido al mayor gasto computacional, demasiadas iteraciones para modelos muy pesados.\n",
    "\n",
    "En el siguiente ejemplo, usamos el práctico clase `StratifiedKFold` de scikit-learn para dividir el conjunto de datos de entrenamiento en 10-folds. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compile_metrics: 76.62%\n",
      "compile_metrics: 68.83%\n",
      "compile_metrics: 70.13%\n",
      "compile_metrics: 64.94%\n",
      "compile_metrics: 66.23%\n",
      "compile_metrics: 74.03%\n",
      "compile_metrics: 77.92%\n",
      "compile_metrics: 67.53%\n",
      "compile_metrics: 76.32%\n",
      "compile_metrics: 73.68%\n",
      "71.62% (+/- 4.44%)\n"
     ]
    }
   ],
   "source": [
    "# Validación cruzada ofrece un buen rendimiento en ML, pero en DL es costoso computacionalmente\n",
    "\n",
    "# MLP para el set de Pima Indians con 10-fold de Validación Cruzada (10 iteraciones con k-fold)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Cargamos el set de pima-indians\n",
    "dataset = np.loadtxt('Datasets/pima-indians-diabetes.csv',delimiter=',')\n",
    "# Dividimos los datos en imputs (características) y outputs (clase)\n",
    "X = dataset[:,0:8]\n",
    "Y = dataset[:,8]\n",
    "# Definimos el teste de 10-fold cross validation utilizado\n",
    "kfold = StratifiedKFold(n_splits=10,shuffle=True) # debemos hacer nosotros un bucle que pase por los splits o vueltas (si agg valor de la semilla me saca aleatoriedad, sobreajustándolo ya que aprende sobre lo aprendido bajo la misma separación de datos)\n",
    "cvscores = []\n",
    "for train,test in kfold.split(X,Y): # tenemos que establecer manualmente 10 modelos que vayan partiendo los datos en train/test con kfold, estableciendo sus resultados individualmente(no lo hace Keras)\n",
    "    # creamos el modelo\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12,input_dim=8,activation='relu')) # primera capa oculta (si encontramos una mejor configuración cambiar)\n",
    "    model.add(Dense(8,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    # compilamos el modelo\n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    # entrenamos el modelo\n",
    "    model.fit(X[train],Y[train],epochs=150,batch_size=10,verbose=0) # ponemos verbose=0 para no mostrar los 150 resultados de cada época sino solo mostrar los finales o globales para las 10 iteraciones\n",
    "    # evaluamos el modelo\n",
    "    scores = model.evaluate(X[test],Y[test],verbose=0)\n",
    "    print('%s: %.2f%%' % (model.metrics_names[1],scores[1]*100)) # imprimo el resultado de cada iteración (es accuracy)\n",
    "    # print('Accuracy: %.2f%%' % (scores[1] * 100))  # Imprimir el resultado de cada iteración aclarándo la métrica correspondiente (este es accuracy)\n",
    "    cvscores.append(scores[1]*100) # lo agrego a la lista\n",
    "# Mostramos los resultados globales (p)\n",
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores),np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Se ha tenido que volver a crear el modelo en cada bucle para luego ajustarlo y evaluarlo con los datos del fold. En la próxima lección veremos cómo podemos usar los modelos de Keras de forma nativa con Scikit-learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
